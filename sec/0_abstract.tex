%\begin{abstract}
%Efficient attention mechanisms are essential for scaling large models, yet their quadratic complexity remains a bottleneck. Existing block-sparse attention methods adopt binary retention of key–value (KV) blocks—either fully keeping or discarding them—which leads to substantial information loss under high sparsity.
%We present \textbf{Pyramid Sparse Attention (PSA)}, a universal sparse attention framework applicable to both Vision–Language Models and video generation. Instead of 0/1 masking, PSA introduces multi-level pooled KV representations. Each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating a smooth continuum between full retention and complete pruning. This design mitigates information loss while preserving computational efficiency at high sparsity.
%Across VLM and video generation benchmarks, PSA matches full-attention accuracy at 0.6 sparsity on Qwen2.5-VL~\cite{qwen2025qwen25technicalreport} (Video-MME~\cite{fu2025videommefirstevercomprehensiveevaluation}), surpassing previous state-of-the-art sparse methods. In training-free video generation on Wan2.1-14B~\cite{wan2025wanopenadvancedlargescale}, it achieves higher similarity scores than block sparse attention based methods under high sparsity. When integrated with step distillation on the video generation model CogVideoX-5B~\cite{hong2022cogvideolargescalepretrainingtexttovideo}, our 4-step sparse model not only outperforms the non-sparse 4-step model but also exceeds the 50-step dense baseline on the VBench~\cite{huang2023vbenchcomprehensivebenchmarksuite}, delivering superior efficiency–quality trade-offs.
%\end{abstract}


% Scaling large Vision–Language Models (VLMs) and accelerating video generation are both constrained by the quadratic cost of attention over long contexts. Prior efficient attention methods, such as block-sparse attention, rely on binary masking that either fully retains or discards key–value (KV) blocks, inevitably causing information loss and quality degradation under high sparsity.
% We present Pyramid Sparse Attention (\textbf{PSAttention}), a universal sparse attention framework applicable to both VLMs and video generation. Instead of 0/1 masking, PSAttention introduces multi-level pooled KV representations. Each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating a smooth continuum between full retention and complete pruning. This design mitigates information loss while preserving computational efficiency at high sparsity.
% Across VLM and video generation benchmarks, PSAttention preserves contextual information and visual fidelity, consistently outperforming existing sparse attention baselines and achieving superior efficiency–quality trade-offs.
%Therefore, sparsity has emerged as the dominant paradigm for building efficient attention mechanisms. 
\begin{abstract}

Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. 
This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm.
Current methods typically retain or discard entire key–value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present \textbf{Pyramid Sparse Attention (PSA)}, a versatile module applicable to both video understanding and generation tasks. 
Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution.
Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency–quality trade-offs. Our code and model weights are publicly available at: \url{http://ziplab.co/PSA}


% For video understanding, PSA matches full-attention accuracy at 65\% sparsity on the Video-MME benchmark using Qwen2.5-VL, outperforming prior SOTA sparse methods. For training-free video generation on Wan2.1-14B, it achieves higher similarity scores than existing sparse attention under high sparsity. Integrated with step distillation on CogVideoX-5B, our 4-step sparse model at 85\% sparsity even outperforms 50-step dense baseline on VBench, enabling superior efficiency–quality trade-offs.

\end{abstract}