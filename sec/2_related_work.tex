\section{Related Work}

The quadratic computational and memory cost of standard attention presents a significant bottleneck for processing long sequences, particularly in tasks like video generation~\cite{kong2025hunyuanvideosystematicframeworklarge,wan2025wanopenadvancedlargescale,tan2025dsvexploitingdynamicsparsity,polyak2025moviegencastmedia,hong2022cogvideolargescalepretrainingtexttovideo} and understanding~\cite{qwen2025qwen25technicalreport,zhang2023videollamainstructiontunedaudiovisuallanguage,madan2024foundationmodelsvideounderstanding,xu2021videoclipcontrastivepretrainingzeroshot,yan2021videogptvideogenerationusing}. To overcome this limitation, sparse attention mechanisms have been developed, which apply a mask to the attention matrix to reduce computations. These methods are broadly classified as static or dynamic.

\noindent\textbf{Static sparse attention.}
Static sparsity methods employ predefined, input-agnostic attention patterns~\cite{zhang2025fastvideogenerationsliding,Hassani_2023_CVPR,child2019generatinglongsequencessparse,li2025radialattentiononlogn,xiao2024efficientstreaminglanguagemodels,zhang2023h2oheavyhitteroracleefficient,xiao2024duoattentionefficientlongcontextllm,xiao2024infllmtrainingfreelongcontextextrapolation,chen2025sparsevditunleashingpowersparse}. These include established patterns such as sliding-window attention~\cite{zhang2025fastvideogenerationsliding,Hassani_2023_CVPR,zhang2023h2oheavyhitteroracleefficient,xiao2024infllmtrainingfreelongcontextextrapolation,xiao2024duoattentionefficientlongcontextllm}, attention sink patterns~\cite{zhu2025sampleattentionnearlosslessaccelerationlong,fu2024moamixturesparseattention,xiao2024efficientstreaminglanguagemodels}, and spatiotemporal energy decay patterns~\cite{li2025radialattentiononlogn}. While computationally efficient, the primary drawback of these methods is their inherent rigidity. Because the patterns are fixed and input-agnostic, they risk missing critical long-range dependencies, which can lead to sub-optimal performance and potentially unstable generation quality.

\noindent\textbf{Dynamic sparse attention.}
To address the limitations of static patterns, dynamic sparsity methods were introduced. These methods generate an \textit{input-sensitive} mask $M$ during the forward pass, for instance, by thresholding attention scores~\cite{zhang2025vsafastervideodiffusion,gu2025bladeblocksparseattentionmeets,xu2025xattentionblocksparseattention,yang2025sparsevideogen2acceleratevideo,xi2025sparsevideogenacceleratingvideo,lu2025mobamixtureblockattention,yuan2025nativesparseattentionhardwarealigned,song2025videonsanativesparseattention}. However, while these content-aware, element-wise adaptive masks offer greater flexibility, they introduce a new challenge: hardware inefficiency. The unstructured, sparse matrix operations resulting from these fine-grained masks lead to poor memory access patterns and low hardware utilization, particularly on parallel processors like GPUs.

\noindent\textbf{Block sparse attention.}
The challenge of hardware utilization motivated the development of Block Sparse Attention (BSA)~\cite{dao2023flashattention2fasterattentionbetter,dao2022flashattentionfastmemoryefficientexact,xu2025xattentionblocksparseattention,gu2025bladeblocksparseattentionmeets,guo2024blocksparse}. This approach introduces a hardware-aware sparse structure by partitioning the $Q, K, V$ and attention matrices into coarse-grained blocks. A binary block mask $M$ then dictates whether an entire block's computation is performed or skipped. By operating at this block level, BSA preserves the dense matrix operations within each block, which is essential for achieving high throughput on modern GPUs. However, BSA still relies on a rigid, binary decision at the block level, which can lead to significant information loss under high sparsity. In contrast, our work introduces a multi-level pooled KV pyramid, allowing each query to access a substantially larger receptive field under the same computational budget and critically mitigating the information loss endemic to high-sparsity binary masking. Moreover, our design does not rely on token reordering, ensuring versatile and seamless applicability across both video understanding and generation models while maintaining high GPU throughput via an efficient kernel.