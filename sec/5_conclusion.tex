\section{Conclusion}
We propose Pyramid Sparse Attention (PSA), a versatile sparse attention mechanism validated across both video understanding and video generation tasks. Our method introduces multi-level sparsity beyond binary keep/skip choices, where higher sparsity levels use larger pooling degrees of KV representations; in this way, unlike block-sparse attention that fully drops low-score blocks, we preserve substantially more information under the same compute budget. As a result, PSA demonstrates strong generalization and consistently outperforms all existing Block Sparse Attention based methods under low compute budget, highlighting its robustness across diverse  model architectures and application scenarios. 
% Furthermore, we introduce a hardware-friendly mergeâ€“split strategy that decouples logical block sizes from hardware tile sizes, effectively addressing the performance challenges posed by dynamic KV block sizes on GPUs.
% While larger-scale pretraining or finetuning experiments are not yet explored due to current resource constraints, extending our method to such settings remains a valuable  direction  for future work.